# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I08bXkFrzPIsxZRhERzu4HaoVYLp8s5b

# Sistem Rekomendasi Buku

DBS Coding Camp
- Zuhair Nashif Abdurrohim
- 1301223102
- MC012D5Y1127

# Import

Kode ini mengimpor pustaka untuk analisis data, pemrosesan file, dan pembelajaran mesin, termasuk TF-IDF untuk representasi teks dan cosine similarity untuk mengukur kesamaan antar teks.
"""

import pandas as pd
import numpy as np
import os
import zipfile
from google.colab import files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf

"""# Data Loading

Mengambil data dari kaggle
- Upload kaggle.json untuk API kaggle
- Ekstract data
- Rubah menjadi dataframe
"""

# Upload file kaggle.json
files.upload()

# Setup untuk API kaggle
os.makedirs("/root/.kaggle", exist_ok=True)
os.rename("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 600)

# Download dataset dari Kaggle
!kaggle datasets download -d arashnic/book-recommendation-dataset

"""Kode ini mengekstrak file ZIP yang berisi dataset rekomendasi buku, kemudian membaca tiga file CSV—**Books.csv**, **Users.csv**, dan **Ratings.csv**—ke dalam **DataFrame** menggunakan **pandas** untuk analisis data lebih lanjut."""

# Ekstrak file ZIP
with zipfile.ZipFile("/content/book-recommendation-dataset.zip", 'r') as zip_ref:
    zip_ref.extractall("book-recommendation-dataset")

# Import dataset ke DataFrame dan tampilkan
books  = pd.read_csv('/content/book-recommendation-dataset/Books.csv')
users  = pd.read_csv('/content/book-recommendation-dataset/Users.csv')
ratings = pd.read_csv('/content/book-recommendation-dataset/Ratings.csv')

"""# Data Understanding
tahap awal proyek untuk mengetahui atau memahami data yang dimiliki

- **Users:** Berisi data pengguna. ID pengguna (User-ID) telah dianonimkan dan dikonversi menjadi angka. Data demografi seperti lokasi dan usia disertakan jika tersedia, tetapi jika tidak, nilainya akan **NULL**.

- **Books:** Setiap buku diidentifikasi berdasarkan ISBN-nya. ISBN yang tidak valid telah dihapus dari dataset. Informasi berbasis konten seperti **judul buku, nama penulis, tahun terbit, dan penerbit** diperoleh dari Amazon Web Services. Jika ada lebih dari satu penulis, hanya penulis pertama yang dicantumkan. URL yang mengarah ke sampul buku tersedia dalam tiga ukuran berbeda (**kecil, sedang, besar**) dan menunjuk ke situs Amazon.

- **Ratings:** Berisi informasi tentang rating buku. Rating (Book-Rating) bisa berupa **rating eksplisit** dalam skala **1-10** (semakin tinggi menunjukkan apresiasi lebih besar) atau **rating implisit** yang ditunjukkan dengan nilai **0**.

Kode ini mencetak jumlah baris dalam masing-masing **DataFrame** untuk dataset buku, pengguna, dan rating, memberikan gambaran tentang ukuran dataset yang digunakan dalam analisis.
"""

print("Jumlah data pada file Books.csv:", books.shape[0])
print("Jumlah data pada file Users.csv:", users.shape[0])
print("Jumlah data pada file Ratings.csv:", ratings.shape[0])

"""# Univariate EDA
melakukan analisis dan eksplorasi setiap variabel data, memahami keterkaitan antar variable

Menampilkan informasi data users
"""

users.info()

"""Menampilkan deskripsi data users"""

users.describe()

"""Menampilkan informasi data book"""

books.info()

"""Menampilkan deskripsi data buku"""

books.describe()

"""Menampilkan informasi data rating"""

ratings.info()

"""Menampilkan deskripsi data rating"""

ratings.describe()

"""# Data Preprocessing
mempersiapkan data sebelum digunakan

Menghapus variabel yang tidak diperlukan (Image dari data books)
"""

books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

"""Tampilkan head data"""

books.head()

"""Menghitung missing value pada data books"""

books.isnull().sum()

"""Menghitung missing value pada data users"""

users.isnull().sum()

"""Menghitung missing value pada data ratings"""

ratings.isnull().sum()

"""Menggabungkan data users, ratings dan books"""

# Gabungkan ratings dengan users
merge_df = pd.merge(ratings, users, on='User-ID', how='left')

# Gabungkan merge_df dengan books
merge_df = pd.merge(merge_df, books, on='ISBN', how='left')

merge_df.shape[0]

merge_df.head()

"""# Data Preparation
mempersiapkan data, mengatasi missing value

Menghitung missing value
"""

merge_df.isnull().sum()

"""Dari hasil jumalh missing value, maka dapat disipulkan bersal dari :
- Rating untuk ISBN tanpa metadata buku
- User tanpa data usia

Tangani missing value metadata buku
"""

# Hapus data dengan metadata tidak lengkap
merge_df = merge_df.dropna(
    subset=[
      'Book-Title',
      'Book-Author',
      'Year-Of-Publication',
      'Publisher'
    ],
    how='any'
)

merge_df.isnull().sum()

"""Karena umur dirasa tidak begitu penting, maka akan dilakukan drop kolom Age"""

merge_df.drop(['Age', 'Location', 'Year-Of-Publication', 'Publisher'], axis=1, inplace=True)

merge_df.isnull().sum()

"""✅ Data bersih dari missing value

# Model Development Content Based Filtering
mengembangkan sistem rekomendasi dengan teknik content based filtering. Teknik content based filtering akan merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu. Pada tahap ini, akan menemukan representasi fitur penting dari setiap kategori buku dengan tfidf vectorizer dan menghitung tingkat kesamaan dengan cosine similarity. Setelah itu, akan membuat sejumlah rekomendasi nuku untuk pelanggan berdasarkan kesamaan yang telah dihitung sebelumnya.

Mengambil 10.000 baris pertama dari merge_df dan menyimpannya dalam variabel data
"""

data = merge_df.head(10000)

"""TF-IDF Vectorizer

Kode ini membuat objek **TfidfVectorizer** untuk mengubah teks menjadi representasi numerik berbasis **TF-IDF**. Kemudian, model dihitung menggunakan **judul buku** (`Book-Title`) sebagai fitur, dan hasilnya digunakan untuk mendapatkan daftar kata yang digunakan dalam proses pemetaan ke indeks numerik.
"""

# Buat TFidfVektorizer
tf = TfidfVectorizer()

# Hitung idf pada title
tf.fit(data['Book-Title'])

# Mapping index integer ke nama
tf.get_feature_names_out()

"""Kode ini mengubah judul buku (`Book-Title`) menjadi **matriks TF-IDF** menggunakan `TfidfVectorizer`. Hasilnya disimpan dalam `tfidf_matrix`, yang merepresentasikan setiap judul buku sebagai vektor numerik berdasarkan bobot TF-IDF. **`tfidf_matrix.shape`** digunakan untuk melihat ukuran matriks, menunjukkan jumlah buku dan jumlah fitur unik dalam teks."""

tfidf_matrix = tf.fit_transform(data['Book-Title'])

tfidf_matrix.shape

"""Kode ini mengubah **matriks TF-IDF** menjadi bentuk **matriks densitas penuh**, yaitu merepresentasikan nilai-nilai TF-IDF dalam format matriks tanpa kompresi, sehingga lebih mudah untuk dianalisis atau divisualisasikan."""

tfidf_matrix.todense()

"""Kode ini membuat **DataFrame** dari **matriks TF-IDF**, dengan kata-kata unik sebagai kolom dan penulis buku sebagai indeks. Kemudian, dilakukan pengambilan sampel acak terhadap **10.661 fitur (kata-kata)** dan **10 penulis**, sehingga hanya sebagian kecil data yang ditampilkan untuk analisis."""

# Dataframe tf-idf matrix

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data['Book-Author']
).sample(10661, axis=1).sample(10, axis=0)

"""Cosine Similarity

Kode ini menghitung kesamaan antar buku menggunakan **cosine similarity** berdasarkan matriks TF-IDF, menghasilkan matriks kesamaan di mana setiap nilai menunjukkan seberapa mirip satu buku dengan lainnya.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Kode ini membuat **DataFrame** dari matriks **cosine similarity**, dengan **Book-Author** sebagai indeks dan kolom. Ini memungkinkan analisis kesamaan antar buku berdasarkan penulisnya. Kemudian, ukuran DataFrame ditampilkan, dan sampel acak **5 kolom** serta **10 baris** diambil untuk melihat sebagian kecil data."""

# Dataframe cosine_sim
cosine_sim_df = pd.DataFrame(
    cosine_sim,
    index=data['Book-Author'],
    columns=data['Book-Author']
)
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Mendapatkan Rekomendasi

Fungsi ini membuat sistem rekomendasi buku berbasis **cosine similarity** dengan langkah-langkah berikut:

- **Mengambil indeks kesamaan**: Menggunakan `argpartition` untuk menemukan `k` buku yang paling mirip dengan penulis yang diberikan.
- **Menentukan buku terdekat**: Memilih buku-buku dengan nilai kesamaan tertinggi berdasarkan hasil dari matriks **cosine similarity**.
- **Menghapus buku input dari hasil**: Menghindari rekomendasi buku yang sama dengan yang diberikan pengguna.
- **Menggabungkan hasil dengan informasi buku**: Mengembalikan DataFrame berisi judul dan penulis dari rekomendasi.
"""

def book_recommendations(book_author, similarity_data=cosine_sim_df, items=data[['Book-Title', 'Book-Author']], k=10):

  index = similarity_data.loc[:,book_author].to_numpy().argpartition(
        range(-1, -k, -1))

  closest = similarity_data.columns[index[-1:-(k+2):-1]]

  closest = closest.drop(book_author, errors='ignore')

  return pd.DataFrame(closest).merge(items).head(k)

"""Kode ini mengambil semua baris dalam `data` yang memiliki nilai **'Tracey West'** di kolom **'Book-Author'**, memungkinkan analisis atau pemfilteran buku berdasarkan penulisnya."""

data[data['Book-Author'].eq('Tracey West')]

"""# Model Development Collaborative Filtering
Model merekomendasikan sejumlah buku berdasarkan rating yang telah diberikan sebelumnya. Dari data rating pengguna, kita akan mengidentifikasi buku-buku yang mirip dan belum pernah dibaca oleh pengguna untuk direkomendasikan.

# Train

Kode ini mendefinisikan `dc` sebagai **DataFrame** untuk sistem rekomendasi berbasis **Collaborative Filtering**, menggunakan dataset rating (`ratings`) yang berisi informasi tentang pengguna dan buku yang mereka nilai.
"""

# dc = datafram collaborative
dc = ratings

"""Kode ini mengambil semua **User-ID unik** dari dataset `dc`, kemudian melakukan **encoding** dengan mengubah **User-ID** menjadi angka yang lebih mudah digunakan dalam model pembelajaran mesin. Selanjutnya, dibuat **mapping decoding** untuk mengubah kembali angka tersebut menjadi **User-ID asli**, memungkinkan konversi dua arah antara format numerik dan ID pengguna."""

# Ambil semua user ID unik dari data
user_ids = dc['User-ID'].unique()

# Encoding: dari user ID asli ke angka
user_encoded = {user_id: idx for idx, user_id in enumerate(user_ids)}

# Decoding: dari angka ke user ID asli
user_decode = {idx: user_id for user_id, idx in user_encoded.items()}

"""Kode ini mengambil semua **ISBN unik** dari dataset `dc`, kemudian melakukan **encoding** dengan mengubah **ISBN** menjadi angka yang lebih mudah digunakan dalam model pembelajaran mesin. Selanjutnya, dibuat **mapping decoding** untuk mengubah kembali angka tersebut menjadi **ISBN asli**, memungkinkan konversi dua arah antara format numerik dan ISBN buku."""

# Ambil semua ISBN unik dari data
book_ids = dc['ISBN'].unique()

# Encoding: dari ISBN asli ke angka
book_encoded = {isbn: idx for idx, isbn in enumerate(book_ids)}

# Decoding: dari angka ke ISBN asli
book_decode = {idx: isbn for isbn, idx in book_encoded.items()}

"""Kode ini melakukan pemetaan (mapping) **User-ID** ke indeks numerik dalam `dc` menggunakan `user_encoded`, serta memetakan **ISBN** ke indeks numerik menggunakan `book_encoded`. Ini bertujuan untuk menyederhanakan data sehingga dapat digunakan dalam model pembelajaran mesin untuk rekomendasi buku."""

# Mapping User-ID ke dataframe user
dc['user'] = dc['User-ID'].map(user_encoded)

# Mapping ISBN ke dataframe buku
dc['book'] = dc['ISBN'].map(book_encoded)

"""Kode ini menghitung jumlah **pengguna** dan **buku** dalam dataset rekomendasi, kemudian mengonversi rating buku menjadi tipe data **float** untuk memastikan kompatibilitas dalam pemrosesan numerik. Selain itu, kode ini juga menentukan **nilai minimum** dan **maksimum** dari rating buku yang diberikan oleh pengguna, membantu memahami distribusi rating dalam sistem rekomendasi."""

# Jumlah user
num_users = len(user_encoded)
print("Jumlah user:", num_users)

# Jumlah buku
num_books = len(book_encoded)
print("Jumlah buku:", num_books)

# Convert rating to float
dc['Book-Rating'] = dc['Book-Rating'].values.astype(np.float32)

# Minimum rating
min_rate = min(dc['Book-Rating'])
print("Minimum rating:", min_rate)

# Maximum rating
max_rate = max(dc['Book-Rating'])
print("Maximum rating:", max_rate)

"""Kode ini mengacak urutan data dalam **DataFrame `dc`** dengan menggunakan `sample(frac=1)`, yang memastikan bahwa semua baris dipilih tetapi dalam urutan acak. Parameter **`random_state=42`** digunakan untuk memastikan hasil yang konsisten setiap kali kode dijalankan."""

dc = dc.sample(frac=1, random_state=42)
dc

"""Kode ini mempersiapkan data untuk pelatihan model rekomendasi. **`x`** berisi pasangan **user** dan **book** dalam bentuk array numerik, sedangkan **`y`** berisi **Book-Rating** yang telah dinormalisasi ke rentang **0-1** berdasarkan nilai minimum dan maksimum dalam dataset. Selanjutnya, **90%** data digunakan sebagai **training set** (`x_train`, `y_train`), dan **10% sisanya** digunakan sebagai **validation set** (`x_val`, `y_val`), memungkinkan model untuk belajar dan diuji sebelum penerapan lebih lanjut."""

x = dc[['user', 'book']].values
y = dc['Book-Rating'].apply(lambda x: (x - min_rate) / (max_rate - min_rate)).values

train_indices = int(0.9 * dc.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
print(x, y)

"""Training

Kode ini membangun model **Collaborative Filtering** menggunakan **Neural Network** dengan langkah-langkah berikut:

- **Embedding Layer:** Membangun representasi numerik pengguna (`user_embedding`) dan buku (`book_embedding`) dalam dimensi `embedding_dim = 32`.
- **Flattening:** Mengubah embedding pengguna dan buku menjadi vektor satu dimensi.
- **Dot Product:** Menghitung skor kesamaan antara pengguna dan buku menggunakan operasi dot product.
- **Model Kompilasi:** Model dibuat menggunakan **Keras Functional API**, dengan optimizer **Adam** dan loss function **Mean Squared Error (MSE)** untuk prediksi rating.
- **Training Model:** Data training (`x_train, y_train`) dan validasi (`x_val, y_val`) digunakan untuk melatih model dalam **5 epoch**.
- **Fungsi Rekomendasi:** `recommend_books()` mencari buku yang belum dinilai oleh pengguna, memprediksi rating menggunakan model, dan mengembalikan **10 buku terbaik** berdasarkan prediksi rating.

Sistem ini memungkinkan rekomendasi buku berdasarkan pola rating pengguna lain dengan pendekatan **latent factor model** menggunakan embedding.
"""

embedding_dim = 32

user_input = tf.keras.layers.Input(shape=(1,), name='user_input')
user_embedding = tf.keras.layers.Embedding(num_users, embedding_dim, name='user_embedding')(user_input)
user_vec = tf.keras.layers.Flatten(name='FlattenUsers')(user_embedding)

book_input = tf.keras.layers.Input(shape=(1,), name='book_input')
book_embedding = tf.keras.layers.Embedding(num_books, embedding_dim, name='book_embedding')(book_input)
book_vec = tf.keras.layers.Flatten(name='FlattenBooks')(book_embedding)

prod = tf.keras.layers.dot([user_vec, book_vec], axes=1, normalize=False)
model = tf.keras.Model([user_input, book_input], prod)
model.compile('adam', 'mean_squared_error')

# Assuming x_train, y_train, x_val, y_val are defined from the previous code
history = model.fit([x_train[:, 0], x_train[:, 1]], y_train,
                    epochs=5,
                    verbose=1,
                    validation_data=([x_val[:, 0], x_val[:, 1]], y_val))


def recommend_books(user_id, dc_df, books_df, k=10):
    # Encode the user ID
    encoded_user_id = user_encoded.get(user_id)

    if encoded_user_id is None:
        print(f"User ID {user_id} not found in the training data.")
        return pd.DataFrame() # Return empty DataFrame

    # Get books already rated by the user
    books_rated_by_user = dc_df[dc_df['User-ID'] == user_id]['ISBN'].tolist()

    # Get all unique book ISBNs from the original books data
    all_book_isbns = books_df['ISBN'].unique()

    # Filter out books already rated by the user
    books_to_predict = [isbn for isbn in all_book_isbns if isbn not in books_rated_by_user]

    if not books_to_predict:
        print(f"User ID {user_id} has rated all available books or no books found to predict.")
        return pd.DataFrame()

    # Encode the books to predict
    encoded_books_to_predict = np.array([book_encoded.get(isbn) for isbn in books_to_predict if book_encoded.get(isbn) is not None])

    if encoded_books_to_predict.size == 0:
         print(f"Could not encode any books to predict for user ID {user_id}.")
         return pd.DataFrame()

    # Create user input array for prediction
    user_input_predict = np.full(len(encoded_books_to_predict), encoded_user_id)

    # Predict ratings for the books the user hasn't rated
    predictions = model.predict([user_input_predict, encoded_books_to_predict])

    # Get the indices of top k predicted ratings
    top_indices = np.argsort(predictions.flatten())[::-1][:k]

    # Get the encoded book IDs of the top recommendations
    top_encoded_book_ids = encoded_books_to_predict[top_indices]

    # Decode the book IDs to ISBNs
    recommended_book_isbns = [book_decode.get(encoded_id) for encoded_id in top_encoded_book_ids]

    # Get book information for the recommended ISBNs
    recommended_books_info = books_df[books_df['ISBN'].isin(recommended_book_isbns)]

    return recommended_books_info[['ISBN', 'Book-Title', 'Book-Author']]

"""Kode ini memilih satu **User-ID** secara acak dari data rating (`dc`), lalu menggunakan fungsi **`recommend_books()`** untuk mendapatkan **10 rekomendasi buku** berdasarkan informasi rating yang diberikan pengguna lain. Hasil rekomendasi ditampilkan dengan format yang mencantumkan **penulis dan judul buku**, atau pesan alternatif jika tidak ditemukan rekomendasi untuk pengguna tersebut."""

# Ambil salah satu user ID dari data rating yang sudah dimuat sebelumnya
# Pastikan user_id ini ada di data training (dc)
sample_user_id = dc['User-ID'].sample(1).iloc[0] # Ambil user ID acak dari data rating

# Panggil fungsi rekomendasi
# Gunakan DataFrame 'dc' untuk user's rated books and 'books' for book info
recommended_books_df = recommend_books(sample_user_id, dc, books, k=10)

# Tampilkan hasil rekomendasi
print(f"Rekomendasi Buku untuk User ID {sample_user_id}:")
print("===" * 10)

if not recommended_books_df.empty:
    for index, row in recommended_books_df.iterrows():
        print(f"{row['Book-Author']} : {row['Book-Title']}")
else:
    print("Tidak ada rekomendasi yang ditemukan untuk pengguna ini.")

"""# Evaluasi Model

Pada bagian ini, kita akan mengevaluasi kinerja dari kedua model rekomendasi yang telah dibangun: Content-Based Filtering dan Collaborative Filtering.

"""

# Fungsi untuk mendapatkan indeks buku berdasarkan judul
def get_book_index(title, data):
    # Menggunakan .str.contains() dengan case=False untuk pencarian tidak case-sensitive
    # dan mengembalikan indeks dari baris pertama yang cocok
    return data[data['Book-Title'].str.contains(title, case=False, na=False)].index.min()

# Fungsi untuk mendapatkan rekomendasi berdasarkan judul buku
def recommend_by_title(book_title, data=data, similarity_data=cosine_sim_df, k=10):
    book_index = get_book_index(book_title, data)

    if book_index is None:
        print(f"Buku dengan judul '{book_title}' tidak ditemukan dalam dataset terbatas ini.")
        return pd.DataFrame()

    # Dapatkan nilai kesamaan untuk buku yang dicari
    similarity_scores = cosine_sim[book_index]

    # Dapatkan indeks buku yang diurutkan berdasarkan skor kesamaan (descending)
    # Ambil k+1 buku pertama (termasuk buku itu sendiri)
    top_indices = similarity_scores.argsort()[::-1][1:k+1] # Mulai dari 1 untuk mengecualikan buku input

    # Dapatkan ISBN dari buku-buku yang direkomendasikan
    recommended_isbns = data.iloc[top_indices]['ISBN'].tolist()

    # Dapatkan informasi buku dari DataFrame asli (books)
    recommended_books_info = books[books['ISBN'].isin(recommended_isbns)]

    return recommended_books_info[['ISBN', 'Book-Title', 'Book-Author']]

# Contoh Evaluasi Content-Based Filtering
print("=== Evaluasi Content-Based Filtering ===")
sample_book_title = "The Lovely Bones" # Ganti dengan judul buku yang ada di dataset terbatas

# Dapatkan rekomendasi untuk buku contoh
recommended_books_cb = recommend_by_title(sample_book_title, data)

if not recommended_books_cb.empty:
    print(f"\nRekomendasi Berdasarkan Konten untuk Buku '{sample_book_title}':")
    print("---" * 10)
    for index, row in recommended_books_cb.iterrows():
        print(f"{row['Book-Author']} : {row['Book-Title']}")
else:
    print(f"\nTidak dapat memberikan rekomendasi berdasarkan konten untuk buku '{sample_book_title}'.")

# Menampilkan nilai Cosine Similarity (Contoh)
# Kita bisa menampilkan matriks kesamaan untuk buku contoh dengan buku-buku lain.
# Karena matriksnya besar, kita ambil contoh buku yang ditemukan
book_index_for_sim = get_book_index(sample_book_title, data)

if book_index_for_sim is not None:
    print(f"\nNilai Cosine Similarity untuk '{sample_book_title}' dengan beberapa buku lain (dataset terbatas):")
    print("---" * 10)

    # Ambil baris kesamaan untuk buku yang dicari
    similarity_row = cosine_sim[book_index_for_sim]

    # Buat DataFrame sementara untuk menampilkan nilai kesamaan
    sim_df = pd.DataFrame({'Book-Title': data['Book-Title'], 'Similarity': similarity_row})

    # Urutkan berdasarkan kesamaan (descending) dan tampilkan beberapa teratas (kecuali buku itu sendiri)
    print(sim_df.sort_values(by='Similarity', ascending=False).head(11).tail(10)) # Ambil 11, buang yang pertama
else:
    print(f"\nTidak dapat menampilkan nilai Cosine Similarity karena buku '{sample_book_title}' tidak ditemukan di dataset terbatas.")


### Evaluasi Collaborative Filtering

# Evaluasi model Collaborative Filtering dapat menggunakan metrik seperti Mean Squared Error (MSE) atau Root Mean Squared Error (RMSE) dari prediksi rating.
# Kita juga bisa melihat contoh rekomendasi untuk user dan membandingkannya dengan buku yang sudah dirating user.

print("\n=== Evaluasi Collaborative Filtering ===")

# Tampilkan loss (MSE) dari proses training dan validasi
print(f"Training Loss (MSE): {history.history['loss'][-1]:.4f}")
print(f"Validation Loss (MSE): {history.history['val_loss'][-1]:.4f}")

# RMSE adalah akar kuadrat dari MSE
train_rmse = np.sqrt(history.history['loss'][-1])
val_rmse = np.sqrt(history.history['val_loss'][-1])

print(f"Training RMSE: {train_rmse:.4f}")
print(f"Validation RMSE: {val_rmse:.4f}")

# Tampilkan kembali contoh rekomendasi untuk user (sudah dilakukan di bagian sebelumnya)
# Kita bisa mengambil user_id yang sama atau user_id lain
sample_user_id_eval = dc['User-ID'].sample(1).iloc[0]

print(f"\nRekomendasi Buku Collaborative Filtering untuk User ID {sample_user_id_eval}:")
print("---" * 10)
recommended_books_cf = recommend_books(sample_user_id_eval, dc, books, k=10)

if not recommended_books_cf.empty:
    for index, row in recommended_books_cf.iterrows():
        print(f"{row['Book-Author']} : {row['Book-Title']}")
else:
    print("Tidak ada rekomendasi Collaborative Filtering yang ditemukan untuk pengguna ini.")

# Untuk evaluasi yang lebih mendalam, kita bisa membagi data menjadi training, validasi, dan testing set.
# Kemudian menghitung metrik pada testing set.
# Contoh: Hitung MSE pada x_val, y_val menggunakan model.evaluate()
# mse_val = model.evaluate([x_val[:, 0], x_val[:, 1]], y_val, verbose=0)
# print(f"\nManual Validation MSE: {mse_val:.4f}")